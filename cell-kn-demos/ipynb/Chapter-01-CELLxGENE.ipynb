{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 01: Use of CELLxGENE\n",
    "\n",
    "Ray LeClair \\<2025-01-08 Wed\\>\n",
    "\n",
    "## Objectives\n",
    "\n",
    "Since CELLxGENE serves as an initiating data source for the NLM Cell\n",
    "Knowledge Network pilot, the objectives of this document include\n",
    "demonstration of:\n",
    "\n",
    "-   Identification of CELLxGENE datasets for specified organisms, and\n",
    "    tissues\n",
    "\n",
    "-   Identification of publications corresponding to CELLxGENE datasets\n",
    "\n",
    "-   Determination of the dataset filename and dataset file download\n",
    "\n",
    "### Background\n",
    "\n",
    "All single-cell RNA data from Chan Zuckerberg (CZ) CELLxGENE Discover\n",
    "is accessed, queried, and analyzed using the CELLxGENE Discover\n",
    "Census. Using cell-based slicing and querying one can:\n",
    "\n",
    "-   Interact with the data through TileDB-SOMA\n",
    "\n",
    "-   Get slices in AnnData, Seurat, or SingleCellExperiment objects\n",
    "\n",
    "See: [CELLxGENE Discover Census](https://chanzuckerberg.github.io/cellxgene-census/)\n",
    "\n",
    "The following sections draw from CZ CELLxGENE tutorials, or a Chan\n",
    "Zuckerberg Initiative (CZI) repository, which demonstrate how to:\n",
    "\n",
    "-   [Explore and query the Census in the context of a single tissue, lung](https://chanzuckerberg.github.io/cellxgene-census/notebooks/analysis_demo/comp_bio_explore_and_load_lung_data.html)\n",
    "\n",
    "-   [Query the expression data and cell/gene metadata from the Census, and load them into common in-memory Python objects](https://chanzuckerberg.github.io/cellxgene-census/notebooks/api_demo/census_query_extract.html)\n",
    "\n",
    "-   [Generate a citation string for all datasets contained in a Census slice](https://chanzuckerberg.github.io/cellxgene-census/notebooks/api_demo/census_citation_generation.html)\n",
    "\n",
    "-   [Fetch full metadata for a Dataset](https://github.com/chanzuckerberg/single-cell-curation/blob/0c77179d2e794846861f8109c037b723507959cb/notebooks/curation_api/python_raw/get_dataset.ipynb)\n",
    "\n",
    "The following sections describe various development environments\n",
    "\n",
    "See: [springbok-nlm-kn/README.md](https://github.com/ralatsdc/springbok-nlm-kn/blob/main/README.md)\n",
    "\n",
    "### Jupyter Notebook development environment\n",
    "\n",
    "Launch Jupyter Notebook from a terminal in which `.zshenv` has been\n",
    "sourced, and the virtual environment has been activated.\n",
    "\n",
    "### Emacs Org Mode development environment\n",
    "\n",
    "Launch Emacs from a terminal in which `.zshenv` has been sourced, then\n",
    "evaluate this code block to activate the virtual environment:\n",
    "\n",
    "``` commonlisp\n",
    "(pyvenv-activate \"../../.venv\")\n",
    "```\n",
    "\n",
    "## Identification of CELLxGENE datasets for specified organisms, and tissues\n",
    "\n",
    "Mostly following the first tutorial, we write a function that obtains\n",
    "all datasets, summary cell counts, gene metadata, and, by default,\n",
    "human and mouse lung, eye, and brain cell metadata from the CZ\n",
    "CELLxGENE Census as Pandas DataFrames. Anticipating a time consuming\n",
    "process, the first call of the function writes the DataFrames to\n",
    "`.parquet` files, then, on subsequent calls, it reads the `.parquet`\n",
    "files. In both cases, the resulting DataFrames are returned.\n",
    "\n",
    "To begin, we import modules, and assign module scope variables:"
   ],
   "id": "1e52d78b-58f1-4f68-a50e-1780c24eb44d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "results": "silent",
    "session": "shared",
    "tangle": "../py/CELLxGENE.py"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "from pathlib import Path\n",
    "import re\n",
    "import subprocess\n",
    "from time import sleep\n",
    "from traceback import print_exception\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import cellxgene_census\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "DATA_DIR = \"../data\"\n",
    "\n",
    "CELLXGENE_DOMAIN_NAME = \"cellxgene.cziscience.com\"\n",
    "CELLXGENE_API_URL_BASE = f\"https://api.{CELLXGENE_DOMAIN_NAME}\"\n",
    "CELLXGENE_DIR = f\"{DATA_DIR}/cellxgene\"\n",
    "\n",
    "CELL_KN_DIR = f\"{DATA_DIR}/cell-kn\"\n",
    "\n",
    "HTTPS_SLEEP = 1\n"
   ],
   "id": "bbb20120-b507-49a9-832c-b79cb9ff953c"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we write the function:"
   ],
   "id": "0effb162-a741-4ce3-8d30-a5f088322c69"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "results": "silent",
    "session": "shared",
    "tangle": "../py/CELLxGENE.py"
   },
   "outputs": [],
   "source": [
    "def get_metadata_and_datasets(\n",
    "    organisms=[\"homo_sapiens\", \"mus_musculus\"], tissues=[\"lung\", \"eye\", \"brain\"]\n",
    "):\n",
    "    \"\"\"Use the CZ CELLxGENE Census to obtain all datasets, summary\n",
    "    cell counts, gene metadata, and, by default, human and mouse\n",
    "    lung, eye, and brain cell metadata, then write the resulting\n",
    "    Pandas DataFrames to parquet files, or, if the files exist, read\n",
    "    them.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    organisms : list(str)\n",
    "        List of organisms, default is [\"homo_sapiens\", \"mus_musculus\"]\n",
    "    tissues : list(str)\n",
    "        List of tissues, default is [\"lung\", \"eye\", \"brain\"]\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    datasets : pd.DataFrame\n",
    "        DataFrame containing dataset descriptions\n",
    "    counts : pd.DataFrame\n",
    "        DataFrame containing summary cell counts\n",
    "    var : pd.DataFrame\n",
    "        DataFrame containing gene metadata\n",
    "    obs : pd.DataFrame\n",
    "        DataFrame containing cell metadata\n",
    "    \"\"\"\n",
    "    # Create and write, or read DataFrames\n",
    "    datasets_parquet = f\"{CELL_KN_DIR}/datasets.parquet\"\n",
    "    counts_parquet = f\"{CELL_KN_DIR}/counts.parquet\"\n",
    "    var_parquet = f\"{CELL_KN_DIR}/var.parquet\"\n",
    "    obs_parquet = f\"{CELL_KN_DIR}/obs.parquet\"\n",
    "    if (\n",
    "        not os.path.exists(datasets_parquet)\n",
    "        or not os.path.exists(counts_parquet)\n",
    "        or not os.path.exists(var_parquet)\n",
    "        or not os.path.exists(obs_parquet)\n",
    "    ):\n",
    "        print(\"Opening soma\")\n",
    "        census = cellxgene_census.open_soma(census_version=\"latest\")\n",
    "\n",
    "        print(\"Collecting all datasets\")\n",
    "        datasets = census[\"census_info\"][\"datasets\"].read().concat().to_pandas()\n",
    "\n",
    "        print(\"Collecting summary cell counts\")\n",
    "        counts = (\n",
    "            census[\"census_info\"][\"summary_cell_counts\"].read().concat().to_pandas()\n",
    "        )\n",
    "\n",
    "        var = pd.DataFrame()\n",
    "        for organism in organisms:\n",
    "            print(f\"Collecting gene metadata for {organism}\")\n",
    "            var = pd.concat(\n",
    "                [\n",
    "                    var,\n",
    "                    cellxgene_census.get_var(\n",
    "                        census,\n",
    "                        organism,\n",
    "                    ),\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        obs = pd.DataFrame()\n",
    "        for organism in organisms:\n",
    "            print(f\"Collecting cell metadata for {organism}: {tissues} tissue\")\n",
    "            obs_for_org = cellxgene_census.get_obs(\n",
    "                census,\n",
    "                organism,\n",
    "                value_filter=f\"tissue_general in {tissues} and is_primary_data == True\",\n",
    "            )\n",
    "            obs_for_org[\"organism\"] = organism\n",
    "            obs = pd.concat(\n",
    "                [\n",
    "                    obs,\n",
    "                    obs_for_org,\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        print(\"Closing soma\")\n",
    "        census.close()\n",
    "\n",
    "        print(\"Writing datasets parquet\")\n",
    "        datasets.to_parquet(datasets_parquet)\n",
    "\n",
    "        print(\"Writing summary cell counts parquet\")\n",
    "        counts.to_parquet(counts_parquet)\n",
    "\n",
    "        print(\"Writing gene metadata parquet\")\n",
    "        var.to_parquet(var_parquet)\n",
    "\n",
    "        print(\"Writing cell metadata parquet\")\n",
    "        obs.to_parquet(obs_parquet)\n",
    "\n",
    "    else:\n",
    "\n",
    "        print(\"Reading datasets parquet\")\n",
    "        datasets = pd.read_parquet(datasets_parquet)\n",
    "\n",
    "        print(\"Reading summary cell counts parquet\")\n",
    "        counts = pd.read_parquet(counts_parquet)\n",
    "\n",
    "        print(\"Reading gene metadata parquet\")\n",
    "        var = pd.read_parquet(var_parquet)\n",
    "\n",
    "        print(\"Reading cell metadata parquet\")\n",
    "        obs = pd.read_parquet(obs_parquet)\n",
    "\n",
    "    return datasets, counts, var, obs\n"
   ],
   "id": "000c405d-048c-4e69-a141-4f50b9e1a1db"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then call it to obtain the human and mouse lung, eye, and brain cell\n",
    "metadata and datasets (using exception handling since accessing an\n",
    "external resource), and print the result:"
   ],
   "id": "76388d61-5553-443c-99c5-384251735f5c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "results": "output",
    "session": "shared"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    datasets, counts, var, obs = get_metadata_and_datasets()\n",
    "except Exception as exc:\n",
    "    print_exception(exc)\n",
    "print(f\"datasets:\\n\\ncolumns: {datasets.columns}\\n\\n{datasets}\")\n",
    "print()\n",
    "print(f\"counts:\\n\\ncolumns: {counts.columns}\\n\\n{counts}\")\n",
    "print()\n",
    "print(f\"var:\\n\\ncolumns: {var.columns}\\n\\n{var}\")\n",
    "print()\n",
    "print(f\"obs:\\n\\ncolumns: {obs.columns}\\n\\n{obs}\")\n"
   ],
   "id": "009653f1-0466-4a6a-a026-61a8b7367161"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can merge some of the resulting DataFrames to create a summary\n",
    "DataFrame for export. As before, we will write the summary DataFrame\n",
    "to a `.parquet` file, so that later we can simply read the `.parquet`\n",
    "file."
   ],
   "id": "daf6a21e-4f18-4415-90f4-97865250ad2b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "results": "output",
    "session": "shared"
   },
   "outputs": [],
   "source": [
    "summary_parquet = f\"{CELL_KN_DIR}/summary.parquet\"\n",
    "if not os.path.exists(summary_parquet):\n",
    "    # Define columns needed from obs\n",
    "    obs_columns = [\n",
    "        \"organism\",\n",
    "        \"tissue_general\",\n",
    "        \"tissue_general_ontology_term_id\",\n",
    "        \"assay\",\n",
    "        \"assay_ontology_term_id\",\n",
    "        \"dataset_id\",\n",
    "    ]\n",
    "\n",
    "    # Define columns required for summary\n",
    "    sum_columns = [\n",
    "        \"organism\",\n",
    "        \"tissue_general\",\n",
    "        \"tissue_general_ontology_term_id\",\n",
    "        \"collection_id\",\n",
    "        \"collection_name\",\n",
    "        \"collection_doi\",\n",
    "        \"assay\",\n",
    "        \"assay_ontology_term_id\",\n",
    "        \"dataset_id\",\n",
    "        \"dataset_title\",\n",
    "        \"dataset_h5ad_path\",\n",
    "    ]\n",
    "\n",
    "    print(\"Merging datasets and obs DataFrames\")\n",
    "    try:\n",
    "        summary = pd.merge(\n",
    "            datasets, obs[obs_columns].drop_duplicates(), on=\"dataset_id\"\n",
    "        )[sum_columns].drop_duplicates()\n",
    "    except Exception as exc:\n",
    "        print_exception(exc)\n",
    "\n",
    "    print(\"Writing summary parquet\")\n",
    "    summary.to_parquet(summary_parquet)\n",
    "\n",
    "    print(\"Writing summary CSV\")\n",
    "    summary_csv = f\"{CELL_KN_DIR}/summary.csv\"\n",
    "    summary.to_csv(summary_csv)\n",
    "\n",
    "else:\n",
    "    print(\"Reading summary parquet\")\n",
    "    summary = pd.read_parquet(summary_parquet)\n"
   ],
   "id": "116f8c11-e0e9-4e23-89fc-813644d7ca2a"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identification of publications corresponding to CELLxGENE datasets\n",
    "\n",
    "We notice that the datasets DataFrame contains a `citation` column,\n",
    "for example:"
   ],
   "id": "4a520f47-95a5-453f-9ae0-fa3254da9092"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "results": "output",
    "session": "shared"
   },
   "outputs": [],
   "source": [
    "print(datasets[\"citation\"].iloc[4])\n"
   ],
   "id": "75b15a9b-57a1-476c-aa40-34effec4fc5e"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `citation` provides the DOI, but not the title of the\n",
    "publication. Note that we will need the title later in Chapter 02:\n",
    "E-Utilities. So, we examine the `collection_name` and `dataset_title`\n",
    "columns:\n",
    "\n",
    "See: [Chapter-02-E-Utilities.ipynb](Chapter-02-E-Utilities.ipynb)"
   ],
   "id": "5138c6f4-e14a-4202-b47b-7c83a9d1dfd2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "results": "output",
    "session": "shared"
   },
   "outputs": [],
   "source": [
    "print(datasets[[\"collection_name\", \"dataset_title\"]].iloc[4, :])\n"
   ],
   "id": "c3ab5578-b78c-4985-a5ba-54ec691fef2c"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But it appears we still need to find the title by some method. So, we\n",
    "write a function that requests the DOI, then parses the resulting\n",
    "page, most likely from the publisher, to find the title."
   ],
   "id": "1da75532-d8d9-4ac5-afd7-e3e8ab519165"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "results": "silent",
    "session": "shared",
    "tangle": "../py/CELLxGENE.py"
   },
   "outputs": [],
   "source": [
    "def get_title(citation):\n",
    "    \"\"\"Get the title given a dataset citation. Note that only wget\n",
    "    succeeded for Cell Press journals, and neither requests nor wget\n",
    "    succeeded for The EMBO Journal and Science.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    citation : str\n",
    "        Dataset citation\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    title : str\n",
    "        Title of publication associated with the dataset\n",
    "    \"\"\"\n",
    "    # Need a default return value\n",
    "    title = None\n",
    "\n",
    "    # Compile patterns for finding the publication URL and article\n",
    "    # title\n",
    "    p1 = re.compile(\"Publication: (.*) Dataset Version:\")\n",
    "    p2 = re.compile(\"articleName : '(.*)',\")\n",
    "\n",
    "    # Assign CSS selectors for selecting article title elements\n",
    "    selectors = [\n",
    "        \"h1.c-article-title\",\n",
    "        \"h1.article-header__title.smaller\",\n",
    "        \"div.core-container h1\",\n",
    "        \"h1.content-header__title.content-header__title--xx-long\",\n",
    "        \"h1#page-title.highwire-cite-title\",\n",
    "    ]\n",
    "\n",
    "    # Find the publication URL\n",
    "    m1 = p1.search(citation)\n",
    "    if not m1:\n",
    "        logging.warning(f\"Could not find citation URL for {citation}\")\n",
    "        return title\n",
    "    citation_url = m1.group(1)\n",
    "    print(f\"Getting title for citation URL: {citation_url}\")\n",
    "\n",
    "    # Attempt to get the publication page using requests\n",
    "    print(f\"Trying requests\")\n",
    "    sleep(HTTPS_SLEEP)\n",
    "    response = requests.get(citation_url)\n",
    "    try_wget = True\n",
    "    if response.status_code == 200:\n",
    "        html_data = response.text\n",
    "\n",
    "        # Got the page, so parse it, and try each selector\n",
    "        fullsoup = BeautifulSoup(html_data, features=\"lxml\")\n",
    "        for selector in selectors:\n",
    "            selected = fullsoup.select(selector)\n",
    "            if selected:\n",
    "\n",
    "                # Selected the article title, so assign it\n",
    "                if len(selected) > 1:\n",
    "                    logging.warning(\n",
    "                        f\"Selected more than one element using {selector} on soup from {citation_url}\"\n",
    "                    )\n",
    "                title = selected[0].text\n",
    "                try_wget = False\n",
    "                break\n",
    "\n",
    "    if try_wget:\n",
    "\n",
    "        # Attempt to get the publication page using wget\n",
    "        print(f\"Trying wget\")\n",
    "        sleep(HTTPS_SLEEP)\n",
    "        completed_process = subprocess.run(\n",
    "            [\"curl\", \"-L\", citation_url], capture_output=True\n",
    "        )\n",
    "        html_data = completed_process.stdout\n",
    "\n",
    "        # Got the page, so parse it, and search for the title\n",
    "        fullsoup = BeautifulSoup(html_data, features=\"lxml\")\n",
    "        found = fullsoup.find_all(\"script\")\n",
    "        if found and len(found) > 4:\n",
    "            m2 = p2.search(found[4].text)\n",
    "            if m2:\n",
    "                title = m2.group(1)\n",
    "\n",
    "    print(f\"Found title: '{title}' for citation URL: {citation_url}\")\n",
    "\n",
    "    return title\n"
   ],
   "id": "5f13b776-5936-47dc-b72a-45c587a7c375"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we call the function for an example citation (again using\n",
    "exception handling since accessing an external resource):"
   ],
   "id": "130ba76e-8ac0-4ff7-a378-114fd1cef93b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "results": "output",
    "session": "shared"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    citation = datasets[\"citation\"].iloc[4]\n",
    "    title = get_title(citation)\n",
    "except Exception as exc:\n",
    "    print_exception(exc)\n"
   ],
   "id": "31abfa0c-7bb1-4a1d-8f67-50fe0fdb1f40"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the function attempts to use `requests`, and if it fails,\n",
    "`wget`, since some publishers respond to one, but not the other. The\n",
    "selectors were discovered by manually inspecting the pages returned\n",
    "for the human lung cell datasets using Google Chrome Developer Tools.\n",
    "\n",
    "## Determine the dataset filename and download the dataset file.\n",
    "\n",
    "Following a notebook found in a CZI repository, we write a function to\n",
    "find the dataset filename, and to download the dataset file, given a\n",
    "row of the datasets DataFrame obtained above:"
   ],
   "id": "0c6da43c-42c9-4470-b8a7-65d8175aeb66"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "results": "silent",
    "session": "shared",
    "tangle": "../py/CELLxGENE.py"
   },
   "outputs": [],
   "source": [
    "def get_and_download_dataset_h5ad_file(dataset_series):\n",
    "    \"\"\"Get the dataset filename and download the dataset file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset_series : pd.Series\n",
    "        A row from the dataset DataFrame\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dataset : str\n",
    "       The dataset filename\n",
    "    \"\"\"\n",
    "    # Need a default return value\n",
    "    dataset_filename = None\n",
    "\n",
    "    # Get the dataset object\n",
    "    collection_id = dataset_series.collection_id\n",
    "    dataset_id = dataset_series.dataset_id\n",
    "    dataset_url = f\"{CELLXGENE_API_URL_BASE}/curation/v1/collections/{collection_id}/datasets/{dataset_id}\"\n",
    "    sleep(HTTPS_SLEEP)\n",
    "    response = requests.get(dataset_url)\n",
    "    response.raise_for_status()\n",
    "    if response.status_code != 200:\n",
    "        logging.error(f\"Could not get dataset for id {dataset_id}\")\n",
    "        return\n",
    "\n",
    "    data = response.json()\n",
    "    if dataset_id != data[\"dataset_id\"]:\n",
    "        logging.error(\n",
    "            f\"Response dataset id: {data['dataset_id']} does not equal specified dataset id: {dataset_id}\"\n",
    "        )\n",
    "        return\n",
    "\n",
    "    # Find H5AD files, if possible\n",
    "    assets = data[\"assets\"]\n",
    "    for asset in assets:\n",
    "        if asset[\"filetype\"] != \"H5AD\":\n",
    "            continue\n",
    "\n",
    "        # Found an H5AD file, so download it, if needed\n",
    "        dataset_filename = Path(urlparse(asset[\"url\"]).path).name\n",
    "        dataset_filepath = f\"{CELLXGENE_DIR}/{dataset_filename}\"\n",
    "        if not os.path.exists(dataset_filepath):\n",
    "            print(f\"Downloading dataset file: {dataset_filepath}\")\n",
    "            with requests.get(asset[\"url\"], stream=True) as response:\n",
    "                response.raise_for_status()\n",
    "                with open(dataset_filepath, \"wb\") as df:\n",
    "                    for chunk in response.iter_content(chunk_size=1024 * 1024):\n",
    "                        df.write(chunk)\n",
    "            print(f\"Dataset file: {dataset_filepath} downloaded\")\n",
    "\n",
    "        else:\n",
    "            print(f\"Dataset file: {dataset_filepath} exists\")\n",
    "\n",
    "    return dataset_filename\n"
   ],
   "id": "61bae572-cbb3-427f-97d0-7db2cb032761"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then call it using the first row of the datasets DataFrame obtained\n",
    "above, and print the result (we'll use exception handling when\n",
    "accessing an external resource from now on):"
   ],
   "id": "d0af3550-162a-4ab2-aa3a-76f934fa4ff6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "results": "output",
    "session": "shared"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    dataset_series = datasets.iloc[4]\n",
    "    get_and_download_dataset_h5ad_file(dataset_series)\n",
    "except Exception as exc:\n",
    "    print_exception(exc)\n"
   ],
   "id": "7750f648-8f90-4d26-80bb-15978407c7c6"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, in Chapter 02 we write functions to search PubMed for the title\n",
    "and identifiers.\n",
    "\n",
    "See: [Chapter-02-E-Utilities.ipynb](Chapter-02-E-Utilities.ipynb)"
   ],
   "id": "b9ccf8fe-517f-4125-9b5c-f912b69f7b3e"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
